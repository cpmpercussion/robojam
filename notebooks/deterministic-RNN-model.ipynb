{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Line Predictor\n",
    "\n",
    "The idea of this script is to test the 'accuracy' of a baseline predictor that simply predicts the same value as the prevoius one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "import keras\n",
    "# little path hack to get robojam from one directory up in the filesystem.\n",
    "from context import * # imports robojam\n",
    "# import robojam # alternatively do this.\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "input_colour = 'darkblue'\n",
    "gen_colour = 'firebrick'\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_df_to_array(perf_df):\n",
    "    \"\"\"Converts a dataframe of a performance into array a,b,dt format.\"\"\"\n",
    "    perf_df['dt'] = perf_df.time.diff()\n",
    "    perf_df.dt = perf_df.dt.fillna(0.0)\n",
    "    # Clean performance data\n",
    "    # Tiny Performance bounds defined to be in [[0,1],[0,1]], edit to fix this.\n",
    "    perf_df.set_value(perf_df[perf_df.dt > 5].index, 'dt', 5.0)\n",
    "    perf_df.set_value(perf_df[perf_df.dt < 0].index, 'dt', 0.0)\n",
    "    perf_df.set_value(perf_df[perf_df.x > 1].index, 'x', 1.0)\n",
    "    perf_df.set_value(perf_df[perf_df.x < 0].index, 'x', 0.0)\n",
    "    perf_df.set_value(perf_df[perf_df.y > 1].index, 'y', 1.0)\n",
    "    perf_df.set_value(perf_df[perf_df.y < 0].index, 'y', 0.0)\n",
    "    return np.array(perf_df[['x', 'y', 'dt']])\n",
    "\n",
    "\n",
    "def perf_array_to_df(perf_array):\n",
    "    \"\"\"Converts an array of a performance (a,b,dt format) into a dataframe.\"\"\"\n",
    "    perf_array = perf_array.T\n",
    "    perf_df = pd.DataFrame({'x': perf_array[0], 'y': perf_array[1], 'dt': perf_array[2]})\n",
    "    perf_df['time'] = perf_df.dt.cumsum()\n",
    "    perf_df['z'] = 38.0\n",
    "    # As a rule of thumb, could classify taps with dt>0.1 as taps, dt<0.1 as moving touches.\n",
    "    perf_df['moving'] = 1\n",
    "    perf_df.set_value(perf_df[perf_df.dt > 0.1].index, 'moving', 0)\n",
    "    perf_df = perf_df.set_index(['time'])\n",
    "    return perf_df[['x', 'y', 'z', 'moving']]\n",
    "\n",
    "\n",
    "def random_touch():\n",
    "    \"\"\"Generate a random tiny performance touch.\"\"\"\n",
    "    return np.array([np.random.rand(), np.random.rand(), 0.01])\n",
    "\n",
    "\n",
    "def constrain_touch(touch):\n",
    "    \"\"\"Constrain touch values from the MDRNN\"\"\"\n",
    "    touch[0] = min(max(touch[0], 0.0), 1.0)  # x in [0,1]\n",
    "    touch[1] = min(max(touch[1], 0.0), 1.0)  # y in [0,1]\n",
    "    touch[2] = max(touch[2], 0.001)  # dt # define minimum time step\n",
    "    return touch\n",
    "\n",
    "def generate_random_tiny_performance(model, first_touch, time_limit=5.0, steps_limit=1000, temp=1.0, model_file=None):\n",
    "    \"\"\"Generates a tiny performance up to 5 seconds in length.\"\"\"\n",
    "    time = 0\n",
    "    steps = 0\n",
    "    previous_touch = first_touch\n",
    "    performance = [previous_touch.reshape((3,))]\n",
    "    while (steps < steps_limit and time < time_limit):\n",
    "        previous_touch = model.predict(previous_touch.reshape(1,1,3))\n",
    "        output_touch = previous_touch.reshape(3,)\n",
    "        output_touch = constrain_touch(output_touch)\n",
    "        performance.append(output_touch.reshape((3,)))\n",
    "        steps += 1\n",
    "        time += output_touch[2]\n",
    "    return np.array(performance)\n",
    "\n",
    "\n",
    "def condition_and_generate(model, perf, time_limit=5.0, steps_limit=1000, temp=1.0, model_file=None):\n",
    "    \"\"\"Conditions the network on an existing tiny performance, then generates a new one.\"\"\"\n",
    "    time = 0\n",
    "    steps = 0\n",
    "    # condition\n",
    "    for touch in perf:\n",
    "        previous_touch = model.predict(touch.reshape(1,1,3))\n",
    "    output = [previous_touch.reshape((3,))]\n",
    "    while (steps < steps_limit and time < time_limit):\n",
    "        previous_touch = model.predict(previous_touch.reshape(1,1,3))\n",
    "        output_touch = previous_touch.reshape(3,)\n",
    "        output_touch = constrain_touch(output_touch)\n",
    "        output.append(output_touch.reshape((3,)))\n",
    "        steps += 1\n",
    "        time += output_touch[2]\n",
    "    net_output = np.array(output)\n",
    "    return net_output\n",
    "\n",
    "def divide_performance_into_swipes(perf_df):\n",
    "    \"\"\"Divides a performance into a sequence of swipe dataframes for plotting.\"\"\"\n",
    "    touch_starts = perf_df[perf_df.moving == 0].index\n",
    "    performance_swipes = []\n",
    "    remainder = perf_df\n",
    "    for att in touch_starts:\n",
    "        swipe = remainder.iloc[remainder.index < att]\n",
    "        performance_swipes.append(swipe)\n",
    "        remainder = remainder.iloc[remainder.index >= att]\n",
    "    performance_swipes.append(remainder)\n",
    "    return performance_swipes\n",
    "\n",
    "def plot_2D(perf_df, name=\"foo\", saving=False):\n",
    "    \"\"\"Plot a 2D representation of a performance 2D\"\"\"\n",
    "    swipes = divide_performance_into_swipes(perf_df)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=gen_colour, linewidth=5.0)\n",
    "    plt.ylim(1.0,0)\n",
    "    plt.xlim(0,1.0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if saving:\n",
    "        plt.savefig(name+\".png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_double_2d(perf1, perf2, name=\"foo\", saving=False):\n",
    "    \"\"\"Plot two performances in 2D\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    swipes = divide_performance_into_swipes(perf1)\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=input_colour, linewidth=5.0)\n",
    "    swipes = divide_performance_into_swipes(perf2)\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=gen_colour, linewidth=5.0)\n",
    "    plt.ylim(1.0,0)\n",
    "    plt.xlim(0,1.0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if saving:\n",
    "        plt.savefig(name+\".png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters:\n",
    "SEQ_LEN = 30\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_UNITS = 64\n",
    "EPOCHS = 30\n",
    "VAL_SPLIT=0.2\n",
    "\n",
    "# These settings train for 2.1 epochs which is pretty good!\n",
    "SEED = 2345  # 2345 seems to be good.\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# tf.set_random_seed(5791)  # only works for current graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-overlapping examples: 138658\n",
      "Done initialising loader.\n",
      "X: (138658, 30, 3) y: (138658, 3)\n",
      "Train X: (110926, 30, 3) y: (110926, 3)\n",
      "Test X: (27732, 30, 3) y: (27732, 3)\n"
     ]
    }
   ],
   "source": [
    "microjam_data_file_name = \"../datasets/TinyPerformanceCorpus.h5\"\n",
    "metatone_data_file_name = \"../datasets/MetatoneTinyPerformanceRecords.h5\"\n",
    "\n",
    "with h5py.File(microjam_data_file_name, 'r') as data_file:\n",
    "    microjam_corpus = data_file['total_performances'][:]\n",
    "with h5py.File(metatone_data_file_name, 'r') as data_file:\n",
    "    metatone_corpus = data_file['total_performances'][:]\n",
    "\n",
    "# sequence_loader = robojam.sample_data.SequenceDataLoader(num_steps=SEQ_LEN + 1, batch_size=BATCH_SIZE, corpus=microjam_corpus, overlap=False)\n",
    "sequence_loader = robojam.sample_data.SequenceDataLoader(num_steps=SEQ_LEN + 1, batch_size=BATCH_SIZE, corpus=metatone_corpus, overlap=False)\n",
    "\n",
    "X, y = sequence_loader.seq_to_singleton_format()\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "\n",
    "import sklearn.model_selection\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Train X:\", X_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Test X:\", X_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 30, 64)            17408     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 50,627\n",
      "Trainable params: 50,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = keras.Sequential()\n",
    "encoder.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(None,SEQ_LEN,3), return_sequences=True))\n",
    "encoder.add(keras.layers.LSTM(HIDDEN_UNITS))\n",
    "encoder.add(keras.layers.Dense(3, activation='relu'))\n",
    "encoder.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110926 samples, validate on 27732 samples\n",
      "Epoch 1/30\n",
      "110926/110926 [==============================] - 55s 492us/step - loss: 0.0218 - val_loss: 0.0229\n",
      "Epoch 2/30\n",
      "110926/110926 [==============================] - 59s 536us/step - loss: 0.0217 - val_loss: 0.0229\n",
      "Epoch 3/30\n",
      "110926/110926 [==============================] - 56s 504us/step - loss: 0.0216 - val_loss: 0.0231\n",
      "Epoch 4/30\n",
      "110926/110926 [==============================] - 58s 526us/step - loss: 0.0216 - val_loss: 0.0230\n",
      "Epoch 5/30\n",
      "110926/110926 [==============================] - 54s 490us/step - loss: 0.0215 - val_loss: 0.0230\n",
      "Epoch 6/30\n",
      "110926/110926 [==============================] - 66s 596us/step - loss: 0.0215 - val_loss: 0.0230\n",
      "Epoch 7/30\n",
      "110926/110926 [==============================] - 73s 660us/step - loss: 0.0214 - val_loss: 0.0228\n",
      "Epoch 8/30\n",
      "110926/110926 [==============================] - 64s 577us/step - loss: 0.0213 - val_loss: 0.0234\n",
      "Epoch 9/30\n",
      "110926/110926 [==============================] - 55s 493us/step - loss: 0.0211 - val_loss: 0.0221\n",
      "Epoch 10/30\n",
      "110926/110926 [==============================] - 57s 510us/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 11/30\n",
      "110926/110926 [==============================] - 55s 492us/step - loss: 0.0203 - val_loss: 0.0215\n",
      "Epoch 12/30\n",
      "110926/110926 [==============================] - 55s 494us/step - loss: 0.0201 - val_loss: 0.0210\n",
      "Epoch 13/30\n",
      "110926/110926 [==============================] - 54s 488us/step - loss: 0.0199 - val_loss: 0.0211\n",
      "Epoch 14/30\n",
      "110926/110926 [==============================] - 57s 516us/step - loss: 0.0198 - val_loss: 0.0210\n",
      "Epoch 15/30\n",
      "110926/110926 [==============================] - 54s 491us/step - loss: 0.0197 - val_loss: 0.0209\n",
      "Epoch 16/30\n",
      "110926/110926 [==============================] - 71s 643us/step - loss: 0.0196 - val_loss: 0.0208\n",
      "Epoch 17/30\n",
      "110926/110926 [==============================] - 64s 577us/step - loss: 0.0195 - val_loss: 0.0207\n",
      "Epoch 18/30\n",
      "110926/110926 [==============================] - 55s 500us/step - loss: 0.0193 - val_loss: 0.0206\n",
      "Epoch 19/30\n",
      "110926/110926 [==============================] - 72s 646us/step - loss: 0.0193 - val_loss: 0.0205\n",
      "Epoch 20/30\n",
      "110926/110926 [==============================] - 73s 660us/step - loss: 0.0191 - val_loss: 0.0206\n",
      "Epoch 21/30\n",
      "110926/110926 [==============================] - 54s 491us/step - loss: 0.0190 - val_loss: 0.0206\n",
      "Epoch 22/30\n",
      "110926/110926 [==============================] - 56s 504us/step - loss: 0.0189 - val_loss: 0.0205\n",
      "Epoch 23/30\n",
      "110926/110926 [==============================] - 54s 491us/step - loss: 0.0187 - val_loss: 0.0205\n",
      "Epoch 24/30\n",
      "110926/110926 [==============================] - 58s 519us/step - loss: 0.0186 - val_loss: 0.0205\n",
      "Epoch 25/30\n",
      "110926/110926 [==============================] - 54s 490us/step - loss: 0.0184 - val_loss: 0.0202\n",
      "Epoch 26/30\n",
      "110926/110926 [==============================] - 57s 514us/step - loss: 0.0183 - val_loss: 0.0203\n",
      "Epoch 27/30\n",
      "110926/110926 [==============================] - 55s 495us/step - loss: 0.0181 - val_loss: 0.0204\n",
      "Epoch 28/30\n",
      "110926/110926 [==============================] - 56s 503us/step - loss: 0.0180 - val_loss: 0.0230\n",
      "Epoch 29/30\n",
      "110926/110926 [==============================] - 55s 494us/step - loss: 0.0179 - val_loss: 0.0206\n",
      "Epoch 30/30\n",
      "110926/110926 [==============================] - 56s 502us/step - loss: 0.0177 - val_loss: 0.0204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e53afd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(\"baseline_RNN_predictor.h5\")\n",
    "# Good MSE error would be 0.004\n",
    "# Right now training: 0.0177, validation: 0.0204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load from saved.\n",
    "from keras.models import load_model\n",
    "encoder = load_model(\"baseline_RNN_predictor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (1, 1, 64)                17408     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (1, 64)                   33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, 3)                    195       \n",
      "=================================================================\n",
      "Total params: 50,627\n",
      "Trainable params: 50,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = keras.Sequential()\n",
    "decoder.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(1,1,3), return_sequences=True, stateful=True))\n",
    "decoder.add(keras.layers.LSTM(HIDDEN_UNITS, stateful=True))\n",
    "decoder.add(keras.layers.Dense(3, activation='relu'))\n",
    "decoder.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "decoder.summary()\n",
    "\n",
    "\n",
    "decoder.set_weights(encoder.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Trying out the model with some different kinds of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAHICAYAAAAV7wD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e7286a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test just generating single performances.\n",
    "\n",
    "p = generate_random_tiny_performance(decoder, random_touch()) #, time_limit=60.0, steps_limit=10000) \n",
    "plot_2D(perf_array_to_df(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAHICAYAAAAV7wD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bca4320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_2D(perf_array_to_df(sequence_loader.examples[512]))\n",
    "t = random.randint(0,len(sequence_loader.examples))\n",
    "t = random.randint(0,len(X_test))\n",
    "\n",
    "p = condition_and_generate(decoder,sequence_loader.examples[t])\n",
    "plot_double_2d(perf_array_to_df(sequence_loader.examples[t]), perf_array_to_df(p))\n",
    "decoder.reset_states()\n",
    "# plot_double_2d(in_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Maker Function\n",
    "\n",
    "A function that generates a Keras RNN model based on hyperparameters. This is the `charRNN` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker(model, layer_size=64, dropout_rate=0.5, num_layers=1, vocab_size=20, input_length=1, lr=0.01, train_mode=True):\n",
    "    \"\"\"Builds a charRNN model with variable layer size, number of layers, droupout, learning rate, and a training mode.\"\"\"\n",
    "    if train_mode:\n",
    "        stateful = False\n",
    "        input_shape = (None, input_length)\n",
    "    else:\n",
    "        stateful = True\n",
    "        input_shape = (1, input_length)\n",
    "    \n",
    "    # Input embedding\n",
    "    model.add(Embedding(vocab_size, layer_size, input_length=input_length, batch_input_shape=input_shape))\n",
    "              \n",
    "    # LSTM layers + 1\n",
    "    for i in range(num_layers - 1):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(layer_size, return_sequences=True, stateful=stateful))\n",
    "    \n",
    "    # Final LSTM layer\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(layer_size, stateful=stateful))\n",
    "\n",
    "    # Project back to vocabulary\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=lr))\n",
    "    model.summary()\n",
    "\n",
    "# m = Sequential()\n",
    "# model_maker(m, layer_size=128, vocab_size=vocabulary_size, input_length=30, train_mode=True)\n",
    "# m.fit(X, y, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
