{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input, merge\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "from tensorflow.contrib.distributions import Categorical, Mixture, MultivariateNormalDiag\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import h5py\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from context import * # imports robojam\n",
    "# import robojam # alternatively do this.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "input_colour = 'darkblue'\n",
    "gen_colour = 'firebrick'\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_df_to_array(perf_df):\n",
    "    \"\"\"Converts a dataframe of a performance into array a,b,dt format.\"\"\"\n",
    "    perf_df['dt'] = perf_df.time.diff()\n",
    "    perf_df.dt = perf_df.dt.fillna(0.0)\n",
    "    # Clean performance data\n",
    "    # Tiny Performance bounds defined to be in [[0,1],[0,1]], edit to fix this.\n",
    "    perf_df.set_value(perf_df[perf_df.dt > 5].index, 'dt', 5.0)\n",
    "    perf_df.set_value(perf_df[perf_df.dt < 0].index, 'dt', 0.0)\n",
    "    perf_df.set_value(perf_df[perf_df.x > 1].index, 'x', 1.0)\n",
    "    perf_df.set_value(perf_df[perf_df.x < 0].index, 'x', 0.0)\n",
    "    perf_df.set_value(perf_df[perf_df.y > 1].index, 'y', 1.0)\n",
    "    perf_df.set_value(perf_df[perf_df.y < 0].index, 'y', 0.0)\n",
    "    return np.array(perf_df[['x', 'y', 'dt']])\n",
    "\n",
    "\n",
    "def perf_array_to_df(perf_array):\n",
    "    \"\"\"Converts an array of a performance (a,b,dt format) into a dataframe.\"\"\"\n",
    "    perf_array = perf_array.T\n",
    "    perf_df = pd.DataFrame({'x': perf_array[0], 'y': perf_array[1], 'dt': perf_array[2]})\n",
    "    perf_df['time'] = perf_df.dt.cumsum()\n",
    "    perf_df['z'] = 38.0\n",
    "    # As a rule of thumb, could classify taps with dt>0.1 as taps, dt<0.1 as moving touches.\n",
    "    perf_df['moving'] = 1\n",
    "    perf_df.set_value(perf_df[perf_df.dt > 0.1].index, 'moving', 0)\n",
    "    perf_df = perf_df.set_index(['time'])\n",
    "    return perf_df[['x', 'y', 'z', 'moving']]\n",
    "\n",
    "\n",
    "def random_touch():\n",
    "    \"\"\"Generate a random tiny performance touch.\"\"\"\n",
    "    return np.array([np.random.rand(), np.random.rand(), 0.01])\n",
    "\n",
    "\n",
    "def constrain_touch(touch):\n",
    "    \"\"\"Constrain touch values from the MDRNN\"\"\"\n",
    "    touch[0] = min(max(touch[0], 0.0), 1.0)  # x in [0,1]\n",
    "    touch[1] = min(max(touch[1], 0.0), 1.0)  # y in [0,1]\n",
    "    touch[2] = max(touch[2], 0.001)  # dt # define minimum time step\n",
    "    return touch\n",
    "\n",
    "def generate_random_tiny_performance(model, first_touch, time_limit=5.0, steps_limit=1000, temp=1.0, model_file=None):\n",
    "    \"\"\"Generates a tiny performance up to 5 seconds in length.\"\"\"\n",
    "    time = 0\n",
    "    steps = 0\n",
    "    previous_touch = first_touch\n",
    "    performance = [previous_touch.reshape((3,))]\n",
    "    while (steps < steps_limit and time < time_limit):\n",
    "        previous_touch = model.predict(previous_touch.reshape(1,1,3))\n",
    "        output_touch = previous_touch.reshape(3,)\n",
    "        output_touch = constrain_touch(output_touch)\n",
    "        performance.append(output_touch.reshape((3,)))\n",
    "        steps += 1\n",
    "        time += output_touch[2]\n",
    "    return np.array(performance)\n",
    "\n",
    "\n",
    "def condition_and_generate(model, perf, time_limit=5.0, steps_limit=1000, temp=1.0, model_file=None):\n",
    "    \"\"\"Conditions the network on an existing tiny performance, then generates a new one.\"\"\"\n",
    "    time = 0\n",
    "    steps = 0\n",
    "    # condition\n",
    "    for touch in perf:\n",
    "        previous_touch = model.predict(touch.reshape(1,1,3))\n",
    "    output = [previous_touch.reshape((3,))]\n",
    "    while (steps < steps_limit and time < time_limit):\n",
    "        previous_touch = model.predict(previous_touch.reshape(1,1,3))\n",
    "        output_touch = previous_touch.reshape(3,)\n",
    "        output_touch = constrain_touch(output_touch)\n",
    "        output.append(output_touch.reshape((3,)))\n",
    "        steps += 1\n",
    "        time += output_touch[2]\n",
    "    net_output = np.array(output)\n",
    "    return net_output\n",
    "\n",
    "def divide_performance_into_swipes(perf_df):\n",
    "    \"\"\"Divides a performance into a sequence of swipe dataframes for plotting.\"\"\"\n",
    "    touch_starts = perf_df[perf_df.moving == 0].index\n",
    "    performance_swipes = []\n",
    "    remainder = perf_df\n",
    "    for att in touch_starts:\n",
    "        swipe = remainder.iloc[remainder.index < att]\n",
    "        performance_swipes.append(swipe)\n",
    "        remainder = remainder.iloc[remainder.index >= att]\n",
    "    performance_swipes.append(remainder)\n",
    "    return performance_swipes\n",
    "\n",
    "def plot_2D(perf_df, name=\"foo\", saving=False):\n",
    "    \"\"\"Plot a 2D representation of a performance 2D\"\"\"\n",
    "    swipes = divide_performance_into_swipes(perf_df)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=gen_colour, linewidth=5.0)\n",
    "    plt.ylim(1.0,0)\n",
    "    plt.xlim(0,1.0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if saving:\n",
    "        plt.savefig(name+\".png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_double_2d(perf1, perf2, name=\"foo\", saving=False):\n",
    "    \"\"\"Plot two performances in 2D\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    swipes = divide_performance_into_swipes(perf1)\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=input_colour, linewidth=5.0)\n",
    "    swipes = divide_performance_into_swipes(perf2)\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=gen_colour, linewidth=5.0)\n",
    "    plt.ylim(1.0,0)\n",
    "    plt.xlim(0,1.0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if saving:\n",
    "        plt.savefig(name+\".png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(Layer):\n",
    "    def __init__(self, output_dim, num_mix, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.num_mix = num_mix\n",
    "        with tf.name_scope('MDN'):\n",
    "            self.mdn_mus     = Dense(self.num_mix * self.output_dim, name='mdn_mus') # mix*output vals, no activation\n",
    "            self.mdn_sigmas  = Dense(self.num_mix * self.output_dim, activation=K.exp, name='mdn_sigmas') # mix*output vals exp activation\n",
    "            self.mdn_pi      = Dense(self.num_mix, activation=K.softmax, name='mdn_pi') # mix vals, softmax\n",
    "        super(MDN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mdn_mus.build(input_shape)\n",
    "        self.mdn_sigmas.build(input_shape)\n",
    "        self.mdn_pi.build(input_shape)\n",
    "        self.trainable_weights = self.mdn_mus.trainable_weights + self.mdn_sigmas.trainable_weights + self.mdn_pi.trainable_weights\n",
    "        self.non_trainable_weights = self.mdn_mus.non_trainable_weights + self.mdn_sigmas.non_trainable_weights + self.mdn_pi.non_trainable_weights\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        m = self.mdn_mus(x)\n",
    "        s = self.mdn_sigmas(x)\n",
    "        p = self.mdn_pi(x)\n",
    "        with tf.name_scope('MDN'):\n",
    "            mdn_out = keras.layers.concatenate([m, s, p], name='mdn_outputs')\n",
    "        return mdn_out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {'output_dim': self.output_dim,                                    \n",
    "#                   'num_mix': self.num_mix}\n",
    "#         base_config = super(MDN, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixture_loss_func(output_dim, num_mixes):\n",
    "    \"\"\"Construct a loss functions for the MDN layer parametrised by number of mixtures.\"\"\"\n",
    "    \n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def loss_func(y_true, y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim, \n",
    "                                                                         num_mixes * output_dim, \n",
    "                                                                         num_mixes], \n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "        mixture = Mixture(cat=cat, components=coll)\n",
    "        loss = mixture.log_prob(y_true)\n",
    "        loss = tf.negative(loss)\n",
    "        return loss\n",
    "    \n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDN'):\n",
    "        return loss_func\n",
    "    \n",
    "def get_mixture_sampling_fun(output_dim, num_mixes):\n",
    "    \"\"\"Construct a sampling function for the MDN layer parametrised by mixtures and output dimension.\"\"\"\n",
    "        \n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def sampling_func(y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim, \n",
    "                                                                         num_mixes * output_dim, \n",
    "                                                                         num_mixes], \n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "        mixture = Mixture(cat=cat, components=coll)\n",
    "        samp = mixture.sample()\n",
    "        # Todo: temperature adjustment for sampling function.\n",
    "        return samp\n",
    "    \n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "        return sampling_func\n",
    "    \n",
    "def get_mixture_mse_accuracy(output_dim, num_mixes):\n",
    "    \"\"\"Construct an MSE accuracy function for the MDN layer \n",
    "    that takes one sample and compares to the true value.\"\"\"\n",
    "    \n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def mse_func(y_true, y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim, \n",
    "                                                                         num_mixes * output_dim, \n",
    "                                                                         num_mixes], \n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "        mixture = Mixture(cat=cat, components=coll)\n",
    "        samp = mixture.sample()\n",
    "        mse = tf.reduce_mean(tf.square(samp - y_true), axis=-1)\n",
    "        # Todo: temperature adjustment for sampling functon.\n",
    "        return mse\n",
    "    \n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "        return mse_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters:\n",
    "SEQ_LEN = 30\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_UNITS = 64\n",
    "EPOCHS = 100\n",
    "VAL_SPLIT=0.2\n",
    "\n",
    "# These settings train for 2.1 epochs which is pretty good!\n",
    "SEED = 2345  # 2345 seems to be good.\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# tf.set_random_seed(5791)  # only works for current graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-overlapping examples: 138658\n",
      "Done initialising loader.\n",
      "X: (138658, 30, 3) y: (138658, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "microjam_data_file_name = \"../datasets/TinyPerformanceCorpus.h5\"\n",
    "metatone_data_file_name = \"../datasets/MetatoneTinyPerformanceRecords.h5\"\n",
    "\n",
    "with h5py.File(microjam_data_file_name, 'r') as data_file:\n",
    "    microjam_corpus = data_file['total_performances'][:]\n",
    "with h5py.File(metatone_data_file_name, 'r') as data_file:\n",
    "    metatone_corpus = data_file['total_performances'][:]\n",
    "\n",
    "# sequence_loader = robojam.sample_data.SequenceDataLoader(num_steps=SEQ_LEN + 1, batch_size=BATCH_SIZE, corpus=microjam_corpus, overlap=False)\n",
    "sequence_loader = robojam.sample_data.SequenceDataLoader(num_steps=SEQ_LEN + 1, batch_size=BATCH_SIZE, corpus=metatone_corpus, overlap=False)\n",
    "\n",
    "X, y = sequence_loader.seq_to_singleton_format()\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 30, 64)            17408     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "mdn_1 (MDN)                  (None, 3)                 2275      \n",
      "=================================================================\n",
      "Total params: 52,707\n",
      "Trainable params: 52,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIMENSION = 3\n",
    "NUMBER_MIXTURES = 5\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(None,SEQ_LEN,OUTPUT_DIMENSION), return_sequences=True))\n",
    "model.add(keras.layers.LSTM(HIDDEN_UNITS))\n",
    "model.add(MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES))\n",
    "model.compile(loss=get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer=keras.optimizers.Adam(), metrics=[get_mixture_mse_accuracy(3,5)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110926 samples, validate on 27732 samples\n",
      "Epoch 1/100\n",
      "110926/110926 [==============================] - 32s 289us/step - loss: -3.2459 - mse_func: 0.1619 - val_loss: -2.1137 - val_mse_func: 0.1180\n",
      "Epoch 2/100\n",
      "110926/110926 [==============================] - 29s 257us/step - loss: -4.2223 - mse_func: 0.0883 - val_loss: -3.6866 - val_mse_func: 0.0884\n",
      "Epoch 3/100\n",
      "110926/110926 [==============================] - 28s 254us/step - loss: -4.0390 - mse_func: 0.1151 - val_loss: -3.2927 - val_mse_func: 0.0935\n",
      "Epoch 4/100\n",
      "110926/110926 [==============================] - 29s 259us/step - loss: -5.3248 - mse_func: 0.0751 - val_loss: -4.4434 - val_mse_func: 0.0781\n",
      "Epoch 5/100\n",
      "110926/110926 [==============================] - 28s 253us/step - loss: -5.1524 - mse_func: 0.0769 - val_loss: -4.1061 - val_mse_func: 0.0816\n",
      "Epoch 6/100\n",
      "110926/110926 [==============================] - 28s 255us/step - loss: -3.5330 - mse_func: 0.0933 - val_loss: -0.4280 - val_mse_func: 0.1050\n",
      "Epoch 7/100\n",
      "110926/110926 [==============================] - 29s 260us/step - loss: -1.1746 - mse_func: 0.0992 - val_loss: -0.6035 - val_mse_func: 0.1011\n",
      "Epoch 8/100\n",
      "110926/110926 [==============================] - 28s 255us/step - loss: -1.2433 - mse_func: 0.0889 - val_loss: -0.3376 - val_mse_func: 0.0902\n",
      "Epoch 9/100\n",
      "110926/110926 [==============================] - 30s 271us/step - loss: -1.6829 - mse_func: 0.0816 - val_loss: -2.0134 - val_mse_func: 0.0725\n",
      "Epoch 10/100\n",
      "110926/110926 [==============================] - 29s 259us/step - loss: -4.2724 - mse_func: 0.0661 - val_loss: -3.4833 - val_mse_func: 0.0706\n",
      "Epoch 11/100\n",
      "110926/110926 [==============================] - 29s 264us/step - loss: -5.3266 - mse_func: 0.0615 - val_loss: -4.9758 - val_mse_func: 0.0648\n",
      "Epoch 12/100\n",
      "110926/110926 [==============================] - 31s 276us/step - loss: -2.6265 - mse_func: 0.0805 - val_loss: 0.4316 - val_mse_func: 0.1054\n",
      "Epoch 13/100\n",
      "110926/110926 [==============================] - 30s 268us/step - loss: -3.5330 - mse_func: 0.0849 - val_loss: 0.4769 - val_mse_func: 0.1467\n",
      "Epoch 14/100\n",
      "110926/110926 [==============================] - 31s 282us/step - loss: -4.7426 - mse_func: 0.0954 - val_loss: -5.3021 - val_mse_func: 0.0825\n",
      "Epoch 15/100\n",
      "110926/110926 [==============================] - 31s 280us/step - loss: -5.3781 - mse_func: 0.0688 - val_loss: -5.1387 - val_mse_func: 0.0669\n",
      "Epoch 16/100\n",
      "110926/110926 [==============================] - 31s 276us/step - loss: -6.0616 - mse_func: 0.0603 - val_loss: -3.6424 - val_mse_func: 0.0754\n",
      "Epoch 17/100\n",
      "110926/110926 [==============================] - 30s 269us/step - loss: -5.5086 - mse_func: 0.0629 - val_loss: -4.0205 - val_mse_func: 0.0663\n",
      "Epoch 18/100\n",
      "110926/110926 [==============================] - 28s 249us/step - loss: -4.8666 - mse_func: 0.0683 - val_loss: -2.4564 - val_mse_func: 0.1221\n",
      "Epoch 19/100\n",
      "110926/110926 [==============================] - 29s 260us/step - loss: -5.3081 - mse_func: 0.0867 - val_loss: -4.7680 - val_mse_func: 0.0880\n",
      "Epoch 20/100\n",
      "110926/110926 [==============================] - 29s 264us/step - loss: -5.7263 - mse_func: 0.0766 - val_loss: -1.6617 - val_mse_func: 0.0899\n",
      "Epoch 21/100\n",
      "110926/110926 [==============================] - 29s 259us/step - loss: -6.4873 - mse_func: 0.0928 - val_loss: -0.6730 - val_mse_func: 0.1668\n",
      "Epoch 22/100\n",
      "110926/110926 [==============================] - 32s 291us/step - loss: -5.1654 - mse_func: 0.0834 - val_loss: -4.4758 - val_mse_func: 0.0779\n",
      "Epoch 23/100\n",
      "110926/110926 [==============================] - 29s 258us/step - loss: -4.0236 - mse_func: 0.0973 - val_loss: -4.8377 - val_mse_func: 0.0622\n",
      "Epoch 24/100\n",
      "110926/110926 [==============================] - 29s 257us/step - loss: -6.1036 - mse_func: 0.0571 - val_loss: -4.3753 - val_mse_func: 0.0749\n",
      "Epoch 25/100\n",
      "110926/110926 [==============================] - 28s 250us/step - loss: -3.9454 - mse_func: 0.0698 - val_loss: -3.7925 - val_mse_func: 0.0693\n",
      "Epoch 26/100\n",
      "110926/110926 [==============================] - 29s 257us/step - loss: -6.2878 - mse_func: 0.0736 - val_loss: -5.2670 - val_mse_func: 0.0852\n",
      "Epoch 27/100\n",
      "110926/110926 [==============================] - 28s 248us/step - loss: -6.6158 - mse_func: 0.0732 - val_loss: -5.5130 - val_mse_func: 0.0832\n",
      "Epoch 28/100\n",
      "110926/110926 [==============================] - 27s 243us/step - loss: -6.4169 - mse_func: 0.0697 - val_loss: -4.5709 - val_mse_func: 0.0796\n",
      "Epoch 29/100\n",
      "110926/110926 [==============================] - 27s 245us/step - loss: -6.3732 - mse_func: 0.0640 - val_loss: -5.1430 - val_mse_func: 0.0714\n",
      "Epoch 30/100\n",
      "110926/110926 [==============================] - 27s 245us/step - loss: -6.8410 - mse_func: 0.0596 - val_loss: -5.3423 - val_mse_func: 0.0670\n",
      "Epoch 31/100\n",
      "110926/110926 [==============================] - 27s 246us/step - loss: -6.4237 - mse_func: 0.0620 - val_loss: -3.6312 - val_mse_func: 0.0800\n",
      "Epoch 32/100\n",
      "110926/110926 [==============================] - 28s 254us/step - loss: -4.2868 - mse_func: 0.0623 - val_loss: -4.7338 - val_mse_func: 0.0705\n",
      "Epoch 33/100\n",
      "110926/110926 [==============================] - 27s 248us/step - loss: -4.8571 - mse_func: 0.0695 - val_loss: -0.6824 - val_mse_func: 0.0813\n",
      "Epoch 34/100\n",
      "110926/110926 [==============================] - 27s 243us/step - loss: -4.1244 - mse_func: 0.0698 - val_loss: -1.1839 - val_mse_func: 0.0758\n",
      "Epoch 35/100\n",
      "110926/110926 [==============================] - 27s 245us/step - loss: -5.9846 - mse_func: 0.0579 - val_loss: -5.2946 - val_mse_func: 0.0664\n",
      "Epoch 36/100\n",
      "110926/110926 [==============================] - 27s 244us/step - loss: -3.3762 - mse_func: 0.0701 - val_loss: -5.1878 - val_mse_func: 0.0678\n",
      "Epoch 37/100\n",
      "110926/110926 [==============================] - 27s 244us/step - loss: -4.1766 - mse_func: 0.0685 - val_loss: -1.5503 - val_mse_func: 0.0904\n",
      "Epoch 38/100\n",
      "110926/110926 [==============================] - 27s 244us/step - loss: -2.5657 - mse_func: 0.0738 - val_loss: -3.9515 - val_mse_func: 0.0718\n",
      "Epoch 39/100\n",
      "110926/110926 [==============================] - 27s 243us/step - loss: -4.4917 - mse_func: 0.0593 - val_loss: -4.8168 - val_mse_func: 0.0723\n",
      "Epoch 40/100\n",
      "110926/110926 [==============================] - 27s 243us/step - loss: -5.3553 - mse_func: 0.0675 - val_loss: -1.6803 - val_mse_func: 0.0746\n",
      "Epoch 41/100\n",
      "110926/110926 [==============================] - 27s 245us/step - loss: -3.2202 - mse_func: 0.0786 - val_loss: -1.8521 - val_mse_func: 0.1157\n",
      "Epoch 42/100\n",
      "110926/110926 [==============================] - 27s 245us/step - loss: -2.4410 - mse_func: 0.1010 - val_loss: -1.9229 - val_mse_func: 0.1175\n",
      "Epoch 43/100\n",
      "110926/110926 [==============================] - 28s 257us/step - loss: -2.6675 - mse_func: 0.0911 - val_loss: -1.9409 - val_mse_func: 0.0982\n",
      "Epoch 44/100\n",
      "110926/110926 [==============================] - 28s 248us/step - loss: -3.1672 - mse_func: 0.0850 - val_loss: -5.2055 - val_mse_func: 0.0904\n",
      "Epoch 45/100\n",
      "110926/110926 [==============================] - 27s 246us/step - loss: -2.8879 - mse_func: 0.0684 - val_loss: -3.7529 - val_mse_func: 0.0783\n",
      "Epoch 46/100\n",
      " 42752/110926 [==========>...................] - ETA: 15s - loss: -3.8553 - mse_func: 0.0654"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 10, 64)            17408     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "mixture_density_4 (MixtureDe (None, 3)                 2275      \n",
      "=================================================================\n",
      "Total params: 52,707\n",
      "Trainable params: 52,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "y_pred: (?, 35)\n",
      "mu: (?, 15)\n",
      "sigma: (?, 15)\n",
      "pi: (?, 5)\n",
      "splits: [3, 3, 3, 3, 3]\n",
      "Mix: tf.distributions.Mixture(\"Mixture\", batch_shape=(?,), event_shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing creation of a mixture model loss function.\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(64, batch_input_shape=(None,10,3), return_sequences=True))\n",
    "model.add(keras.layers.LSTM(64))\n",
    "m = MixtureDensity(3, 5)\n",
    "model.add(m)\n",
    "# model.compile(loss=m.get_loss_func(), optimizer=keras.optimizers.Adam())\n",
    "model.summary()\n",
    "\n",
    "num_mixes = 5\n",
    "output_dim = 3\n",
    "y_pred = model.output\n",
    "# Start messing around with model.output.\n",
    "out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim, num_mixes * output_dim, num_mixes], axis=1, name='mdn_coef_split')\n",
    "cat = Categorical(logits=out_pi)\n",
    "component_splits = [output_dim] * num_mixes\n",
    "mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "    in zip(mus, sigs)]\n",
    "mixture = Mixture(cat=cat, components=coll)\n",
    "\n",
    "print(\"y_pred:\", y_pred.shape)\n",
    "print(\"mu:\",  out_mu.shape)\n",
    "print(\"sigma:\", out_sigma.shape)\n",
    "print(\"pi:\", out_pi.shape)\n",
    "print(\"splits:\", component_splits)\n",
    "print(\"Mix:\", mixture)\n",
    "\n",
    "# MultivariateNormalDiag(loc=out_mu, scale_diag=out_sigma)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
